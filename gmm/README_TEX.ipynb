{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fitting procedure:\n",
    "\n",
    "The objective is to maximise the log-likelihood with respect to the parameters of the $K$ mixtures, $\\theta$. However, this is actually a constrained optimisation problem since our prior weights $\\{\\pi_j\\}_{j=1}^{K}$ need to obey the following \"rules\":\n",
    "\n",
    "$\\forall j:  1\\le j\\le K$ we need $\\pi_j\\ge 0$ and $\\sum_{j=1}^{K}\\pi_j=1$.\n",
    "\n",
    "Hence, it is really a lagrangian that we have to optimise.\n",
    "\n",
    "$\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} log\\left(\\sum_{j=1}^{K} \\pi_j \\mathcal{N}\\left(x_i | \\mu_j, \\Sigma_j\\right)\\right) + \\lambda \\left(\\sum_{j=1}^{K}\\pi_j -1\\right)$\n",
    "\n",
    "where $\\theta = \\left\\{\\left(\\pi_j, \\mu_j, \\Sigma_j\\right)\\right\\}_{j=1}^{K}$ and $\\mathcal{N}\\left(x_i | \\mu_j, \\Sigma_j\\right)$ is \n",
    "the Gaussian density, parameterised by $\\mu_j$ and $\\Sigma_j$, and evaluated at the $d$-dimensional datapoint $x_i$.\n",
    "\n",
    "The first order necessary conditions for the parameters:\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}(\\theta)}{\\pi_j} = 0$\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}(\\theta)}{\\Sigma_j} = 0$\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}(\\theta)}{\\mu_j} = 0$\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}(\\theta)}{\\lambda} = 0$\n",
    "\n",
    "do not yield closed form solutions.\n",
    "In fact, the above lead to:\n",
    "\n",
    "* $\\pi_j = \\frac{\\sum_{i=1}^{N}r_{i,j}}{N}$\n",
    "\n",
    "* $\\Sigma_j=\\frac{\\sum_{i=1}^{N}r_{i, j}(x_i-\\mu_j)(x_i - \\mu_j)^T}{\\sum_{i=1}^{N}r_{i, j}}$\n",
    "\n",
    "* $\\mu_j^T = \\frac{\\sum_{i=1}^{N}r_{i, j}x_i^T}{\\sum_{i=1}^{N}r_{i,j}}$\n",
    "\n",
    "* $\\lambda=-N$\n",
    "\n",
    "where \n",
    "\n",
    "$r_{i, j} = \\frac{\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(x_i|\\mu_j, \\Sigma_j)} \\ge0$\n",
    "\n",
    "and\n",
    "\n",
    "$\\sum_{j=1}^{K}r_{i,j}=1$.\n",
    "\n",
    "The derivation of the above is skipped, but many good texts show how to do it. The reader might wish to consult with Chapter 11 of [MML book](https://mml-book.github.io/) for more details.\n",
    "\n",
    "These $r$'s are known as the \"responsibilities\", or the posterior probability of the $i^{th}$ observation \"coming\" from the $j^{th}$ mixture from a latent variable model's perspective. Since the $r$'s depend on all parameters $\\theta$ we cannot obtain a closed form solution for this optimisation problem in contrast to e.g. Least squares linear regression.\n",
    "\n",
    "The maximisation problem is attempted to be solved, therefore, by an iterative procedure:\n",
    "\n",
    "1. Initialise parameters $\\theta = \\left\\{(\\pi_j, \\mu_j, \\Sigma_j)\\right\\}_{j=1}^{K}$\n",
    "    * In my implementation of the algorithm, I sampled eigenvalues, $\\Lambda_j$ for each covariance matrix from a $\\mathcal{U}(1, 2.5)$ distribution and then multiplied those on both \"sides\" by a Householder reflection matrix which has the general form \n",
    "    \n",
    "        $H=I_n - 2\\frac{vv^T}{<v, v>}$ for any $n$-dimensional vector $v$. This is an orthogonal matrix which allows me to make use of the eigen decomposition of real symmetric matrices and construct \n",
    "        \n",
    "        $\\Sigma_j = H\\Lambda_j H^T$.\n",
    "2. Compute $r_{i,j}$\n",
    "3. Compute $\\pi_j$, $\\Sigma_j$, $\\mu_j$ using the equations above for all $K$ mixtures with the $r$'s from step 2 (don't update the \"responsibilities\" unless you are at step 2).\n",
    "4. Repeat steps 2 and 3 until the change in the log-likelihood becomes \"small enough\".\n",
    "\n",
    "This maximisation procedure is an example of the Expectation Maximisation algorithm (popular with other latent variable models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
